# LLM Incident Copilot - Docker Compose Configuration
#
# INFERENCE OPTIONS (choose one):
# 1. Groq Cloud (FASTEST - recommended): Set GROQ_API_KEY below
#    - Free tier: https://console.groq.com/keys
#    - 500+ tokens/sec, no GPU needed
# 2. Local Ollama: Install Ollama on your machine
#    - Run: ollama pull nomic-embed-text && ollama pull llama3.2:3b
# 3. Docker Ollama: Uncomment ollama service below
#
# REQUIRED FOR EMBEDDINGS (always needed):
#   ollama pull nomic-embed-text   # For embeddings (274 MB)

services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage

  # OPTION 2: Uncomment this section to run Ollama in Docker
  # Note: This requires no local Ollama installation but downloads models inside the container
  # Also change OLLAMA_BASE_URL below from host.docker.internal to ollama
  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

  backend:
    build: ./backend
    environment:
      - QDRANT_URL=http://qdrant:6333
      - COLLECTION_NAME=log_chunks
      # Ollama settings (for embeddings, always required)
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - OLLAMA_EMBED_MODEL=nomic-embed-text
      # LLM options: llama3.2:3b (4GB VRAM), llama3.1 (8GB+ VRAM)
      - OLLAMA_MODEL=llama3.2:3b
      # Get your free API key at https://console.groq.com/keys
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - GROQ_MODEL=llama-3.1-8b-instant
      # Performance settings
      - MAX_CHUNKS=50
      - EMBEDDING_CONCURRENCY=5
      - STORAGE_DIR=/app/storage
      - CORS_ORIGINS=http://localhost:5173
    volumes:
      - backend_storage:/app/storage
    ports:
      - "8000:8000"
    depends_on:
      - qdrant
    extra_hosts:
      - "host.docker.internal:host-gateway"

  frontend:
    build: ./frontend
    ports:
      - "5173:5173"
    environment:
      - VITE_API_BASE=http://localhost:8000
    depends_on:
      - backend

volumes:
  qdrant_data:
  ollama_data:
  backend_storage:

