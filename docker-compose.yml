# LLM Incident Copilot - Docker Compose Configuration
#
# SETUP OPTIONS:
# 1. Local Ollama (recommended): Install Ollama on your machine, then run docker compose up
# 2. Docker Ollama: Uncomment the ollama service below if you want to run Ollama in Docker
#
# REQUIRED MODELS (run these before starting):
#   ollama pull nomic-embed-text   # For embeddings (274 MB)
#   ollama pull llama3.2:3b        # For analysis - works on 4GB+ VRAM (2 GB)
#   OR: ollama pull llama3.1       # For analysis - needs 8GB+ VRAM (4.7 GB)

services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage

  # OPTION 2: Uncomment this section to run Ollama in Docker
  # Note: This requires no local Ollama installation but downloads models inside the container
  # Also change OLLAMA_BASE_URL below from host.docker.internal to ollama
  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

  backend:
    build: ./backend
    environment:
      - QDRANT_URL=http://qdrant:6333
      - COLLECTION_NAME=log_chunks
      # For local Ollama: use host.docker.internal
      # For Docker Ollama: change to http://ollama:11434
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      # Model options: llama3.2:3b (4GB VRAM), llama3.1 (8GB+ VRAM), mistral (8GB VRAM)
      - OLLAMA_MODEL=llama3.2:3b
      - OLLAMA_EMBED_MODEL=nomic-embed-text
      - STORAGE_DIR=/app/storage
      - CORS_ORIGINS=http://localhost:5173
    volumes:
      - backend_storage:/app/storage
    ports:
      - "8000:8000"
    depends_on:
      - qdrant
    # Required for connecting to host machine's Ollama (works on Windows, Mac, and Linux)
    extra_hosts:
      - "host.docker.internal:host-gateway"

  frontend:
    build: ./frontend
    ports:
      - "5173:5173"
    environment:
      - VITE_API_BASE=http://localhost:8000
    depends_on:
      - backend

volumes:
  qdrant_data:
  ollama_data:
  backend_storage:

