# LLM Incident Copilot - Environment Configuration
# ================================================
# Copy this file to .env and update values as needed:
#   cp .env.example .env
#
# The .env file is ignored by git (see .gitignore) so your secrets are safe.

# ===========================
# üöÄ GROQ CLOUD (RECOMMENDED)
# ===========================
# Get your FREE API key at: https://console.groq.com/keys
# Groq provides 500+ tokens/sec inference - 10x faster than local!
GROQ_API_KEY=
# Groq model options:
#   - llama-3.1-70b-versatile  (best quality, slower)
#   - llama-3.1-8b-instant     (fast, good quality) [RECOMMENDED]
#   - mixtral-8x7b-32768       (good for long contexts)
GROQ_MODEL=llama-3.1-8b-instant

# ===========================
# ü¶ô OLLAMA (LOCAL)
# ===========================
# Ollama is always required for embeddings (nomic-embed-text model)
# Also used as fallback LLM if GROQ_API_KEY is not set
OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_MODEL=llama3.2:3b
OLLAMA_EMBED_MODEL=nomic-embed-text

# ===========================
# ‚ö° PERFORMANCE
# ===========================
# Max chunks to process per file (prevents timeouts on very large files)
MAX_CHUNKS=50
# Number of parallel embedding API calls
EMBEDDING_CONCURRENCY=5

# ===========================
# üìÅ FILE LIMITS
# ===========================
# Reject files larger than this (MB)
MAX_FILE_SIZE_MB=50
# Show warning for files larger than this (MB)
WARN_FILE_SIZE_MB=10

# ===========================
# üîß ADVANCED
# ===========================
# Override inference provider: auto, groq, ollama
# "auto" uses Groq if API key is set, otherwise Ollama
INFERENCE_PROVIDER=auto
# CORS origins (comma-separated)
CORS_ORIGINS=http://localhost:5173
# Qdrant vector database URL
QDRANT_URL=http://qdrant:6333
# Collection name for log chunks
COLLECTION_NAME=log_chunks
# Storage directory
STORAGE_DIR=/app/storage

# ===========================
# üé® FRONTEND
# ===========================
VITE_API_BASE=http://localhost:8000

